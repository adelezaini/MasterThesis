{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ba6ae1",
   "metadata": {},
   "source": [
    "# Analysis on output of the simulation VEG_SHIFT_IDEAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ae6581e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import netCDF4\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import glob #return all file paths that match a specific pattern\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys; sys.path.append(\"..\")\n",
    "from dataset_manipulation import fix_cam_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efc8bd3",
   "metadata": {},
   "source": [
    "**NOTE**: If you then copy this to NIRD remember to change the path of the input files and the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b385565f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files found\n"
     ]
    }
   ],
   "source": [
    "# Import \n",
    "raw_path = '../../../archive/' #Betzy: /cluster/home/adelez/nird/ #Nird: /nird/home/adelez/storage/\n",
    "processed_path = '../../processed-data/postprocessing/'\n",
    "\n",
    "casename = 'VEG_SHIFT_IDEAL_2000_sec_nudg_f19_f19'\n",
    "casealias = 'IDEAL-ON'\n",
    "\n",
    "fp = raw_path+casename+'/atm/hist/'+casename+'.cam.h0.*.nc'\n",
    "#VEG_SHIFT_IDEAL_2000_sec_nudg_f19_f19.cam.h0.2007-01.nc\n",
    "all_files = glob.glob(fp)\n",
    "all_files.sort()\n",
    "print(\"Files found\")\n",
    "\n",
    "ds = xr.open_mfdataset(all_files)\n",
    "print(\"Dataset created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ccdd8",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6df44e",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Fix timestamp of model data\n",
    "ds = fix_cam_time(ds)\n",
    "\n",
    "# Remove spinup months of data set\n",
    "ds = ds.isel(time=slice(12,len(ds.time)))\n",
    "\n",
    "print(\"Postprocessing completed\")\n",
    "date = '20082012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bddf367",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# BVOC variables\n",
    "variables = ['SFisoprene', 'SFmonoterp']\n",
    "ds[variables].to_netcdf(processed_path+casealias+'_'+'BVOC_'+date+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18094872",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Av = 6.022e23  # Avogadro's number\n",
    "M_iso = 68.114200e-3  # Molar mass isoprene kg/mol\n",
    "M_mono = 136.228400e-3  # Molar mass monoterpene kg/mol\n",
    "path_this_file = str(pathlib.Path().absolute())\n",
    "\n",
    "var_dic = dict(SFmonoterp='H10H16',\n",
    "            SFisoprene='ISOP')\n",
    "M_dic = dict(SFmonoterp=M_mono,\n",
    "            SFisoprene=M_iso)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92b9d62",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# SOA variables\n",
    "variables = ['N_AER', 'SOA_A1','SOA_NA','cb_SOA_A1','cb_SOA_NA', 'cb_SOA_A1_OCW', 'cb_SOA_NA_OCW']\n",
    "ds[variables].to_netcdf(processed_path+casealias+'_'+'SOA_'+date+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384da90e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# CLOUD PROPERTIES\n",
    "variables = ['ACTNL', 'ACTREL','CDNUMC', 'CLDHGH', 'CLDLOW', 'CLDMED', 'CLDTOT', 'CLDLIQ', 'CLOUD', 'CLOUDCOVER_CLUBB', 'FCTL', 'LWCF', 'SWCF', 'NUMLIQ', 'TGCLDLWP']\n",
    "ds[variables].to_netcdf(processed_path+casealias+'_'+'CLOUDPROP_'+date+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c370a954",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# RADIATIVE COMPONENTS\n",
    "variables = ['FLNT', 'FSNT', 'FLNT_DRF', 'FLNTCDRF', 'FSNTCDRF', 'FSNT_DRF']\n",
    "\n",
    "xr_ds = ds.copy()\n",
    "xr_ds = xr_ds[variables]\n",
    "\n",
    "for var in ['SWDIR_Ghan', 'LWDIR_Ghan', 'DIR_Ghan', 'SWCF_Ghan', 'LWCF_Ghan', 'NCFT_Ghan', 'SW_rest_Ghan', 'LW_rest_Ghan']:\n",
    "    \n",
    "    if ('SWDIR_Ghan' == var):# or ('DIR_Ghan' == var):\n",
    "        xr_ds['SWDIR_Ghan'] = xr_ds['FSNT'] - xr_ds['FSNT_DRF']\n",
    "        xr_ds['SWDIR_Ghan'].attrs['units'] = xr_ds['FSNT_DRF'].attrs['units']\n",
    "    if ('LWDIR_Ghan' == var):# or ('DIR_Ghan' == var):\n",
    "        xr_ds['LWDIR_Ghan'] = -(xr_ds['FLNT'] - xr_ds['FLNT_DRF'])\n",
    "        xr_ds['LWDIR_Ghan'].attrs['units'] = xr_ds['FLNT_DRF'].attrs['units']\n",
    "    if 'DIR_Ghan' == var:\n",
    "        xr_ds['DIR_Ghan'] = xr_ds['LWDIR_Ghan'] + xr_ds['SWDIR_Ghan']\n",
    "        xr_ds['DIR_Ghan'].attrs['units'] = xr_ds['LWDIR_Ghan'].attrs['units']\n",
    "    if 'SWCF_Ghan' == var:\n",
    "        xr_ds['SWCF_Ghan'] = xr_ds['FSNT_DRF'] - xr_ds['FSNTCDRF']\n",
    "        xr_ds[var].attrs['units'] = xr_ds['FSNT_DRF'].attrs['units']\n",
    "    if 'LWCF_Ghan' == var:\n",
    "        xr_ds[var] = -(xr_ds['FLNT_DRF'] - xr_ds['FLNTCDRF'])\n",
    "        xr_ds[var].attrs['units'] = xr_ds['FLNT_DRF'].attrs['units']\n",
    "    if 'NCFT_Ghan' == var:\n",
    "        xr_ds[var] = xr_ds['FSNT_DRF'] - xr_ds['FSNTCDRF'] - (xr_ds['FLNT_DRF'] - xr_ds['FLNTCDRF'])\n",
    "        xr_ds[var].attrs['units'] = xr_ds['FLNT_DRF'].attrs['units']\n",
    "    if 'SW_rest_Ghan' == var:\n",
    "        xr_ds[var] = xr_ds['FSNTCDRF']\n",
    "    elif 'LW_rest_Ghan' == var:\n",
    "        xr_ds[var] = xr_ds['FLNTCDRF']\n",
    "    \n",
    "xr_ds.to_netcdf(processed_path+casealias+'_'+'RADIATIVE_'+date+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26e83748",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# TURBULENT FLUXES\n",
    "variables = ['LHFLX', 'OMEGAT', 'SHFLX']\n",
    "ds[variables].to_netcdf(processed_path+casealias+'_'+'TURBFLUXES_'+date+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1825cc",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfba620",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8ead1f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16611349",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a59785e4",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 17\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Loop through each file in the list\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[1;32m     14\u001b[0m     \n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Load a single dataset\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m#timestep_ds = xr.open_dataset(i)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     timestep_ds \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_mfdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Create a new variable called 'time' from the `time_coverage_start` field, and \u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# convert the string to a datetime object so xarray knows it is time data\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     timestep_ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mstrptime(timestep_ds\u001b[38;5;241m.\u001b[39mtime_coverage_start, \n\u001b[1;32m     22\u001b[0m                                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS.\u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/nird/home/u1/adelez/storage/jupyter_venv/lib/python3.9/site-packages/xarray/backends/api.py:996\u001b[0m, in \u001b[0;36mopen_mfdataset\u001b[0;34m(paths, chunks, concat_dim, compat, preprocess, engine, data_vars, coords, combine, parallel, join, attrs_file, combine_attrs, **kwargs)\u001b[0m\n\u001b[1;32m    993\u001b[0m     open_ \u001b[38;5;241m=\u001b[39m open_dataset\n\u001b[1;32m    994\u001b[0m     getattr_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m\n\u001b[0;32m--> 996\u001b[0m datasets \u001b[38;5;241m=\u001b[39m [open_(p, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopen_kwargs) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m paths]\n\u001b[1;32m    997\u001b[0m closers \u001b[38;5;241m=\u001b[39m [getattr_(ds, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_close\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m datasets]\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preprocess \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/nird/home/u1/adelez/storage/jupyter_venv/lib/python3.9/site-packages/xarray/backends/api.py:996\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    993\u001b[0m     open_ \u001b[38;5;241m=\u001b[39m open_dataset\n\u001b[1;32m    994\u001b[0m     getattr_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m\n\u001b[0;32m--> 996\u001b[0m datasets \u001b[38;5;241m=\u001b[39m [\u001b[43mopen_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m paths]\n\u001b[1;32m    997\u001b[0m closers \u001b[38;5;241m=\u001b[39m [getattr_(ds, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_close\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m datasets]\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preprocess \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/nird/home/u1/adelez/storage/jupyter_venv/lib/python3.9/site-packages/xarray/backends/api.py:545\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    539\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mopen_dataset(\n\u001b[1;32m    540\u001b[0m     filename_or_obj,\n\u001b[1;32m    541\u001b[0m     drop_variables\u001b[38;5;241m=\u001b[39mdrop_variables,\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecoders,\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    544\u001b[0m )\n\u001b[0;32m--> 545\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43m_dataset_from_backend_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite_encoded_chunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43minline_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/nird/home/u1/adelez/storage/jupyter_venv/lib/python3.9/site-packages/xarray/backends/api.py:357\u001b[0m, in \u001b[0;36m_dataset_from_backend_dataset\u001b[0;34m(backend_ds, filename_or_obj, engine, chunks, cache, overwrite_encoded_chunks, inline_array, **extra_tokens)\u001b[0m\n\u001b[1;32m    355\u001b[0m     ds \u001b[38;5;241m=\u001b[39m backend_ds\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 357\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43m_chunk_ds\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackend_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverwrite_encoded_chunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m        \u001b[49m\u001b[43minline_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m ds\u001b[38;5;241m.\u001b[39mset_close(backend_ds\u001b[38;5;241m.\u001b[39m_close)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# Ensure source filename always stored in dataset object\u001b[39;00m\n",
      "File \u001b[0;32m/nird/home/u1/adelez/storage/jupyter_venv/lib/python3.9/site-packages/xarray/backends/api.py:317\u001b[0m, in \u001b[0;36m_chunk_ds\u001b[0;34m(backend_ds, filename_or_obj, engine, chunks, overwrite_encoded_chunks, inline_array, **extra_tokens)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chunk_ds\u001b[39m(\n\u001b[1;32m    309\u001b[0m     backend_ds,\n\u001b[1;32m    310\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_tokens,\n\u001b[1;32m    316\u001b[0m ):\n\u001b[0;32m--> 317\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tokenize\n\u001b[1;32m    319\u001b[0m     mtime \u001b[38;5;241m=\u001b[39m _get_mtime(filename_or_obj)\n\u001b[1;32m    320\u001b[0m     token \u001b[38;5;241m=\u001b[39m tokenize(filename_or_obj, mtime, engine, chunks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_tokens)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dask'"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "\n",
    "# List all matching files\n",
    "fns = 'VEG_SHIFT_IDEAL_2000_sec_nudg_f19_f19.cam.h0.*.nc'\n",
    "files = glob.glob(fn_path+fns)\n",
    "\n",
    "# Create list for \n",
    "individual_files = []\n",
    "\n",
    "# Loop through each file in the list\n",
    "for i in files:\n",
    "    \n",
    "    # Load a single dataset\n",
    "    #timestep_ds = xr.open_dataset(i)\n",
    "    timestep_ds = xr.open_mfdataset(i)\n",
    "    \n",
    "    # Create a new variable called 'time' from the `time_coverage_start` field, and \n",
    "    # convert the string to a datetime object so xarray knows it is time data\n",
    "    timestep_ds['time'] = datetime.strptime(timestep_ds.time_coverage_start, \n",
    "                                           \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    \n",
    "    # Add the dataset to the list\n",
    "    individual_files.append(timestep_ds)\n",
    "\n",
    "# Combine individual datasets into a single xarray along the 'time' dimension\n",
    "modis_ds = xr.concat(individual_files, dim='time')\n",
    "\n",
    "print(modis_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014416d4",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
